# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import math
import torch
import torch.nn as nn


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    """Truncated normal initialization (approximation)
    
    Fills the input Tensor with values drawn from a truncated normal distribution.
    Values are effectively drawn from the normal distribution N(mean, std^2) 
    with values outside [a, b] redrawn until they are within the bounds.
    
    Args:
        tensor: an n-dimensional torch.Tensor
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    """
    def norm_cdf(x):
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # having mean 0 and std 1
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def repeat_interleave_batch(x, B, repeat):
    """
    Repeat interleave batch dimension.
    
    Args:
        x: tensor of shape (B*N, ...)
        B: original batch size
        repeat: number of times to repeat
        
    Returns:
        tensor of shape (B*repeat*N, ...)
    """
    N = len(x) // B
    x = x.repeat(repeat, *([1] * (x.dim() - 1)))
    x = x.reshape(repeat, N, B, *x.shape[1:]).permute(1, 2, 0, *range(3, x.dim() + 1))
    x = x.reshape(N * B * repeat, *x.shape[3:])
    return x


def apply_masks(x, masks):
    """
    Apply masks to input tensor x.
    
    Args:
        x: tensor of shape (B, N, D) where B is batch size, N is sequence length, D is embedding dim
        masks: list of tensors, each of shape (B, n_keep) containing indices to keep
        
    Returns:
        tensor of shape (B * len(masks), n_keep, D) containing masked patches
    """
    all_x = []
    for m in masks:
        # m has shape (B, n_keep) where each row contains indices to keep
        B = x.shape[0]
        # Expand mask for gathering: (B, n_keep) -> (B, n_keep, D)
        mask_expanded = m.unsqueeze(-1).expand(-1, -1, x.shape[-1])
        # Gather the masked patches
        x_masked = torch.gather(x, dim=1, index=mask_expanded)
        all_x.append(x_masked)
    
    # Concatenate along batch dimension
    return torch.cat(all_x, dim=0)
